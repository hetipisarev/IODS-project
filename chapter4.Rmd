---
title: " "
author: "Heti Pisarev"
output:
  html_document: default
  pdf_document: default
---


#  Chapter 4 -- Clustering and classification
```{r echo=FALSE}
date()
```

## 4.1 Data wrangling for chapter 5

### 4.1.2 Read the data

Read the “Human development” and “Gender inequality” datas into R. 

Meta file and some technical notes:

<http://hdr.undp.org/en/content/human-development-index-hdi>
<http://hdr.undp.org/sites/default/files/hdr2015_technical_notes.pdf>

```{r eval=FALSE}
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
ii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
```

### 4.1.3 Explore the datasets

```{r eval=FALSE}
dim(hd); dim(ii)

names(hd)
names(ii)

summary(hd)
summary(ii)
```
### 4.1.4 Rename variables
```{r eval=FALSE}
names(hd) = c("hdi.rank", "country", "hdi", "life.exp.birth", "exp.educ", "mean.educ", "gni", "gni.minus.hdi")

names(ii) = c("gii.rank", "country", "gii", "maternal.mort.ratio", "adoles.birth.rate", "parliament" ,"sec.edu.f", "sec.edu.m", "labour.rate.f", "labour.rate.m"  )
```  
###  4.1.5 Add variables
Add to “Gender inequality” ratio of female and male populations:

* secondary education in each country
* labour force participation

```{r eval=FALSE}
library(dplyr)
ii = mutate(ii, edu.ratio = sec.edu.f/sec.edu.m)
ii = mutate(ii, lab.ratio = labour.rate.f/labour.rate.m)
# First rows of mutated dataset
head(ii, n=3)
```

### 4.1.6 Merge datasets
```{r eval=FALSE}
human <- inner_join(hd, ii, by = "country")

names(human)
dim(human)

# Write data to file
write.csv(human, file = (paste0(getwd(), "/data/human.csv")), row.names=F)
```

***
***

## 4.2 Data analysis

```{r eval=TRUE, error=FALSE, warning=FALSE, message=FALSE}
# Do some cleaning work
rm(list = ls())

# Todays libraries:
library(MASS); library(corrplot); library(tidyverse)
```

### 4.2.2 Load and describe Boston data

Load and explore the dataset Boston (offered in library MASS).
```{r eval=FALSE, error=FALSE, warning=FALSE, message=FALSE}
data("Boston")
str(Boston)                                  
```

Boston dataset  contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. Dataset has `r dim(Boston)[1]` rows and `r dim(Boston)[2]` variables. All variables, except binary _chas_, are numberic. See descriptive statistics on next chapter. 

variables | explanation
----------|--------------
crim  | per capita crime rate by town.
zn   | proportion of residential land zoned for lots over 25,000 sq.ft.
indus   | proportion of non-retail business acres per town.
chas  | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
nox  | nitrogen oxides concentration (parts per 10 million).
rm  | average number of rooms per dwelling.
age  | proportion of owner-occupied units built prior to 1940.
dis  | weighted mean of distances to five Boston employment centres.
rad  | index of accessibility to radial highways.
tax  | full-value property-tax rate per \$10,000.
ptratio  | pupil-teacher ratio by town.
black  | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
lstat  | lower status of the population (percent).
medv  | median value of owner-occupied homes in \$1000s.
 

 
### 4.2.3 Overview of the data
Descriptive statistics:
```{r eval=TRUE, error=FALSE, warning=FALSE, message=FALSE}
summary(Boston)
```
Distribution of variables:
```{r , echo=TRUE}
par(mfrow=c(4,4), mai=c(0.6, 0.6, 0.2, 0.2), cex=0.7)
for (i in 1:14) hist(Boston[,i], main="", xlab=names(Boston)[i], cex=0.7) 
```

Visualized correlation matrix:
```{r , echo=TRUE, fig.width = 4, fig.height = 3}
cor_matrix<-cor(Boston, method="spearman") 
par(mfrow=c(1,1))
corrplot(cor_matrix, method="circle")
# More fancy correlation graph - just for memorize
# corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d" , tl.cex = 0.6)
```

All variables are on very different scale. Most on them are not normally distributed. There are lot of correlations between variables, for example highly negatively are correlated lstat and medv, dis with indus, nox and age. High positive correlation are between nox and indus, age; between indus and tax; between medv and rm.


### 4.2.4 Prepare dataset for discriminant analysis
_**Exercise:** Standardize the dataset and print out summaries of the scaled data. How did the variables change? Create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate). Use the quantiles as the break points in the categorical variable. Drop the old crime rate variable from the dataset. Divide the dataset to train and test sets, so that 80% of the data belongs to the train set._ 

Center and standardize variables, print out summaries.
```{r eval=TRUE}
boston_scaled <- as.data.frame(scale(Boston))

summary(boston_scaled)
```
After centering means of variables are 0.

Use quantiles of variable *crime* as the break points to create a categorical variable. Drop the old crime rate variable from the dataset.
```{r eval=TRUE, fig.width = 5, fig.height = 2}
bins <- quantile(boston_scaled$crim)
bins

boston_scaled$crime <- cut(boston_scaled$crim, breaks = bins, 
             include.lowest = TRUE, 
             label=c("low", "med_low", "med_high", "high"))

table(boston_scaled$crime)

# visualize new vs old crime
par(mai=c(0.6,0.6,0.2,0.2))
plot(log(Boston$crim), boston_scaled$crime, main = "New categorical variable vs original", las=1, cex=0.7)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
```
Divide dataset to training (80%) and testing (20%) number of rows in the Boston dataset. 
```{r eval=TRUE}
# No of rows in dataset
n <- dim(boston_scaled)[1]

# Choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# Create train set
train <- boston_scaled[ind,]

# Create test set 
test <- boston_scaled[-ind,]

```


### 4.2.5 Fit the linear discriminant analysis
_**Exercise:** Fit the linear discriminant analysis on the train set. Use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. 
Draw the LDA (bi)plot._

Fit linear discriminant analysis and print the results and make plot.
```{r eval=TRUE}
lda.fit <- lda(crime ~ ., data = train)
lda.fit

# function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit , dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 1)
```

The output presents distribution of target variable, means on descriptive variables in target variable groups and  coefficients of linear discriminants (needed to make latent variables describing target variable groups).

### 4.2.6  Testing model on test data
_**Exercise:** Save the crime categories from the test set and then remove the categorical crime variable from the test dataset. Then predict the classes with the LDA model on the test data. Cross tabulate the results with the crime categories from the test set. Comment on the  results._

```{r eval=TRUE}
# save the correct classes from test data
test$correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
a = table(correct = test$correct_classes, predicted = lda.pred$class)
a
# correct predictions
sum(a[1,1]+a[2,2]+a[3,3]+a[4,4]) / sum(a)

```

Model classified correctly 69% of observations. It's not totally bad, but it could be better.


### 4.2.7 Cluster analysis
_**Exercise:** Reload the Boston dataset and standardize the dataset (we did not do this in the Datacamp exercises, but you should scale the variables to get comparable distances). Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results._

```{r eval=TRUE,  warning=FALSE, fig.width = 5, fig.height =2}
# Euclidean distance matrix
dist_eu <- dist(boston_scaled)

# Look at the summary of the distances
summary(dist_eu)

# Determine the number of clusters
set.seed(123)
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

According to the last plot 2 cluster is enough.

```{r eval=TRUE,  warning=FALSE}

# k-means clustering
km <-kmeans(dist_eu, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston[1:7], col = km$cluster)

pairs(Boston[8:14], col = km$cluster)

```
I made cluster plots by two graphs.
I think that clusters are more clearly separated by variables medv, lstat, age, rm, nox and crime.

### Bonus
_**Exercise:** Perform k-means on the original Boston data with some reasonable number of clusters (> 2). Remember to standardize the dataset. Then perform LDA using the clusters as target classes. Include all the variables in the Boston data in the LDA model. Visualize the results with a biplot (include arrows representing the relationships of the original variables to the LDA solution). Interpret the results. Which variables are the most influencial linear separators for the clusters?_

```{r}
boston_scaled = as.data.frame(scale(Boston))
dist_eu = dist(boston_scaled)
boston_scaled$target<-kmeans(dist_eu, centers = 3)$cluster
lda.fit <- lda(target  ~ ., data = boston_scaled)
classes <- as.numeric(boston_scaled$target)
plot(lda.fit , dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 1)
lda.pred <- predict(lda.fit, newdata = boston_scaled)
a = table(correct = boston_scaled$target, predicted = lda.pred$class)
a
sum(a[1,1]+a[2,2]+a[3,3]) / sum(a)
```
I did not separate dataset here as trainig and and testing, so the rate of correct answers is high - over 90%.
